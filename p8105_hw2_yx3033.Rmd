---
title: "p8105_hw2_yx3033"
Author: "Yvonne Xie"
Data: "2025-10-01"
output: github_document
---

```{r}
library(tidyverse)
library(dplyr)
library(readxl)
library(lubridate)
```

## Problem 1
Clean pols data set

```{r message=FALSE, warning=FALSE}
pols_df = 
  read_csv("data/pols-month.csv") |> 
  janitor::clean_names() |> 
  separate(mon,into=c("year","month","day")) |> 
  mutate(
    year=as.integer(year),
    month = as.integer(month),
    day = as.integer(day)
  ) |> 
  mutate(month=month.name[month]) |> 
  mutate(president=if_else(prez_gop==1,"gop","dem")) |> 
  select(-prez_dem,-prez_gop,-day)
pols_df
```

clean snv dataset

```{r message=FALSE, warning=FALSE}
snv_df =
  read_csv("data/snp.csv") |> 
  janitor::clean_names() |> 
  separate(date,into=c("month","day","year"), sep="/") |> 
  mutate(
    year = as.integer(if_else(as.integer(year) > 15, paste0("19", year), paste0("20", year))),
    month = as.integer(month),
    day = as.integer(day)
  ) |> 
  mutate(month = month.name[month]) |> 
  select(year, month, close) |> 
  arrange(year, match(month, month.name))
snv_df
```

clean unemployment data set
```{r message=FALSE, warning=FALSE}
unemp_df =
  read_csv("data/unemployment.csv") |> 
  janitor::clean_names() |> 
  pivot_longer(
    cols = jan:dec,
    names_to = "month",
    values_to = "unemployment"
  ) |> 
  mutate(
    month = month.name[match(tolower(month), tolower(month.abb))]
  ) |> 
  select(year, month, unemployment)
unemp_df
```

join snv into pols and merge in unemployment

```{r}
merged_df =
  left_join(pols_df,snv_df, by = c("year", "month")) |> 
  left_join(unemp_df, by = c("year", "month"))
merged_df
```

Description:  
The merged dataset combines three original FiveThirtyEight datasets: `pols-month`, `snp`, and `unemployment`. The `pols-month` dataset contains information on the number of Republican and Democratic politicians, including presidents, governors, senators, and representatives, with 822 monthly observations from 1947 onward. The `snp` dataset contains monthly closing values of the Standard & Poorâ€™s (S&P) 500 index, with 787 observations. The `unemployment` dataset records monthly unemployment rates by year, originally in a wide format with one row per year and separate columns for each month. After tidying and merging, the resulting dataset has 822 rows and 11 key variables: `year`, `month`, `gov_gop`, `sen_gop`, `rep_gop`, `gov_dem`, `sen_dem`, `rep_dem`, `president`, `close` (S&P 500), and `unemployment`. The dataset spans from 1947 through 2015, with consistent month names, which enables analyses that relate to political composition, economic indicators, and labor market conditions over time.  


## Problem 2

```{r}
mr_trash =
  read_excel("data/202509 Trash Wheel Collection Data.xlsx",
             sheet = "Mr. Trash Wheel",
             skip =1) |> 
  janitor::clean_names() |> 
  select(where(~!all(is.na(.)))) |>
  filter(!is.na(dumpster)) |> 
  mutate(year = as.integer(year),
         sports_balls = as.integer(round(sports_balls)),
         trash_wheel = "Mr. Trash Wheel")

mr_trash
```


```{r}
pro_trash = 
  read_excel("data/202509 Trash Wheel Collection Data.xlsx",
             sheet = "Professor Trash Wheel",
             skip = 1) |> 
  janitor::clean_names() |> 
  filter(!is.na(dumpster)) |> 
  mutate(trash_wheel = "Professor Trash Wheel")

gwyn_trash = 
  read_excel("data/202509 Trash Wheel Collection Data.xlsx",
             sheet = "Gwynns Falls Trash Wheel",
             skip = 1) |> 
  janitor::clean_names() |> 
  filter(!is.na(dumpster)) |> 
  mutate(trash_wheel = "Gwynnda")
```

combine all three:
```{r}
all_trash <- bind_rows(mr_trash, pro_trash, gwyn_trash) |> 
  mutate(cigarette_butts = as.integer(round(cigarette_butts)))
```

The trash collection dataset combines records from multiple trash wheels, including Mr. Trash Wheel, Professor Trash Wheel, and Gwynns Falls Trash Wheel spanning from 2014 to 2025. The resulting dataset contains `r nrow(mr_trash)` observations and 15 variables, such as `dumpster` (the dumpster ID), `weight_tons` (weight of trash collected), `cigarette_butts`, `plastic_bottles`, `sports_balls`, `volume_cubic_yards`, `homes_powered` and `trash_wheel` indicating which trash wheel sites is that line of data from. These variables provide types and quantities of waste collected on each date and sites. For example, the total weight of trash collected by Professor Trash Wheel was `r sum(all_trash$weight_tons[all_trash$trash_wheel == "Professor Trash Wheel"], na.rm = TRUE)` tons. The total number of cigarette butts collected by Gwynnda in June of 2022 was `r sum(all_trash$cigarette_butts[all_trash$trash_wheel == "Gwynnda" & all_trash$month == "June" & all_trash$year == 2022], na.rm = TRUE)`. This dataset allows for detailed exploration of waste composition across different trash wheels and at different time periods.


## Problem 3

```{r}
zip_codes=
  read_csv("data/Zip Codes.csv") |> 
  janitor::clean_names() |> 
  mutate(zip_code = as.character(zip_code))
```

deleated 'county' after county Bronx
```{r}
zip_zori =
  read_csv("data/Zip_zori_uc_sfrcondomfr_sm_month_NYC.csv") |> 
  janitor::clean_names() |> 
  mutate(
    county_name = str_remove_all(county_name,"County"),
    county_name = trimws(county_name),
    region_name = as.character(region_name),
  ) |> 
  rename_with(~ str_remove(.x, "^x"))
```

check for duplicates in the zip_codes dataset
```{r}
zip_codes_dupes = zip_codes |> 
  group_by(zip_code)  |> 
  filter(n() > 1)  |> 
  ungroup()
zip_codes_dupes

zip_zori_dupes = zip_zori |> 
  group_by(region_name)  |> 
  filter(n() > 1)  |> 
  ungroup()
zip_zori_dupes

cat("Duplicate zip codes found:", nrow(zip_codes_dupes), "\n")
cat("Duplicate region_names found:", nrow(zip_zori_dupes), "\n")

#ZIP 10463 appears in both Bronx County and New York County (Manhattan)
#ZIP 11201 appears in both Kings County (Brooklyn) and New York County (Manhattan)
```


```{r}

zori_tidy = zip_zori |> 
  pivot_longer(
    cols = `2015_01_31`:`2024_08_31`,
    names_to = "date",
    values_to = "rent_index") |>
  mutate(date = lubridate::ymd(date)) |> 
  filter(!is.na(rent_index)) |> 
  select(-region_type,-state_name
  )
```

merge two
```{r}
final_dataset = zori_tidy |> 
  left_join(
    zip_codes, 
    by = c("region_name" = "zip_code", "county_name" = "county")
  )
```
arrange by importance
```{r}
final_dataset = final_dataset  |> 
  
  mutate(zip_code = region_name,
         borough = county_name) |> 
  select(
    zip_code, 
    borough, 
    neighborhood, date, rent_index,
    city, metro, state, size_rank, 
    everything()
  ) |> 
  arrange(borough, neighborhood, date)

```

total observation, unique zip and neighborhoods
```{r}
total_obs <- nrow(final_dataset)
cat("Total observations:", total_obs, "\n")

unique_zips <- n_distinct(final_dataset$zip_code)
cat("Unique ZIP codes:", unique_zips, "\n")

unique_neighborhoods <- n_distinct(final_dataset$neighborhood)
cat("Unique neighborhoods:", unique_neighborhoods, "\n")
```

zip not in zori
```{r}
missing_in_zori =
  zip_codes |> 
  anti_join(zip_zori, by = c("zip_code" = "region_name"))  |> 
  distinct(zip_code,county, neighborhood)
cat("ZIP codes in zip code data but missing from Zillow Rental Price dataset:\n")
missing_in_zori
```

```{r}
missing_by_borough = missing_in_zori |> 
  group_by(county) |> 
  summarise(
    count = n(),
    example_zips = paste(head(zip_code, 3), collapse = ", "),
    .groups = 'drop'
  )
missing_by_borough
```
The analysis reveals that 171 ZIP codes from the neighborhood dataset are missing from the Zillow rental data, with Manhattan showing the highest number of exclusions (102 ZIP codes), followed by Queens (44), Kings (11), Bronx (8), and Richmond (6). One of the reason that there are so many in Manhattan (New York County) is that for example ZIP code 10008 in Manhattan's Financial District is primarily composed of office buildings, skyscrapers, and commercial spaces with very few residential rental properties. Zillow likely excludes such areas because very few buildings there are in residential rental markets.


Price comparison
```{r}
january_comparison = 
  final_dataset |> 
  filter((lubridate::year(date) == 2020 & lubridate::month(date) == 1) | 
         (lubridate::year(date) == 2021 & lubridate::month(date) == 1)) |> 
  select(zip_code, borough, neighborhood, date, rent_index)  |> 
  pivot_wider(
    names_from = date,
    values_from = rent_index,
    names_prefix = "jan_"
  )  |> 
  filter(!is.na(`jan_2020-01-31`) & !is.na(`jan_2021-01-31`)) |> 
  mutate(
    price_change = `jan_2021-01-31` - `jan_2020-01-31`,
    percent_change = (`jan_2021-01-31` - `jan_2020-01-31`) / `jan_2020-01-31` * 100
  )


largest_drops= 
  january_comparison |> 
  arrange(price_change)  |> 
  head(10)  |> 
  select(
    zip_code, 
    borough, 
    neighborhood, 
    jan_2020_price = `jan_2020-01-31`,
    jan_2021_price = `jan_2021-01-31`, 
    price_drop = price_change,
    percent_drop = percent_change
  ) |> 
  mutate(
    price_drop = round(price_drop, 2),
    percent_drop = round(percent_drop, 1)
  )

largest_drops
```

Based on the table, the largest drop heavily concentrated in New York county (Manhattan), which likely reflects that people are moving out of the city during the pandemic and working remote from home. The top 10 ZIP codes experienced dramatic declines of **`r round(min(largest_drops$percent_drop), 1)`% to `r round(max(largest_drops$percent_drop), 1)`%**, representing substantial financial impacts for property owners in these areas.